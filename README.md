# TF2_BERTJapanese

TensorFlow 2系とHuggingfaceにある東北大の日本語BERTでタスク解きなどを実装練習する。

# 「BERTによる自然言語処理入門 Transformersを使った実践プログラミング」

## classifiy_sentences

文章を読んで、複数のラベルから適切なラベルを一つ選んで文章につける。データセットはLivedoor News Corpusを使う。

疑問点として、「TPUで学習させるとGPUほど正解率が上がらない」というのが残っている。

## multi-labeling

文章を読んで、ラベルをつける。必要であれば複数のラベルをつける。

今回はchABSAデータセットを使い、有価証券報告書を読んで内容がポジティブ、ネガティブ、ニュートラルを含んでいるかどうか判定させる。

マルチラベリングでは、最後の層でソフトマックスでなくシグモイドを使う。各ラベルに属する確率を求めたいが、それぞれの属する確率を足し合わせて1になる保証がないからである。

損失関数は、それぞれのラベルが含まれるか否かの2値になるのでBinary Crossentropyを使うことに注意する。

## extract_ner

文章の各トークンが固有表現なのかどうかを判定する。

## correction_press

漢字の誤表記を訂正するタスクについてファインチューニングする。トークンがボキャブラリに含まれるトークンのどれであるべきなのかという多クラス分類になっている。

## search_similar_sentence

事前学習済みのBERTでLivedoor News Corpusの記事を埋め込むとジャンルで分かれるのかを、PCAとt-SNEで次元圧縮してプロットを見て確認してみる。

また、文章ベクトルの作り方として[CLS]トークンの表現を使う場合と[PAD]でないトークンの表現を平均する場合を採用している。

同じジャンルの記事は内積類似度が高くなるようにBERTを学習させてもいいが、今回はTensorboardによる可視化と次元圧縮による可視化しかしていない。
