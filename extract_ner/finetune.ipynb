{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"extract_ner.ipynb","provenance":[],"toc_visible":true,"collapsed_sections":[],"authorship_tag":"ABX9TyMd54vUEiYPLrKsualwTjqS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#設定"],"metadata":{"id":"C86d39bd0zid"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fL98QIvO0ikj"},"outputs":[],"source":["%%capture\n","!pip install transformers ipadic fugashi"]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import random\n","import itertools\n","import json\n","import unicodedata\n","import datetime\n","import os\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from transformers import BertJapaneseTokenizer, TFBertForTokenClassification"],"metadata":{"id":"YYPRa9A-1BJq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n","MAX_LENGTH = 128"],"metadata":{"id":"lx_ArGi319VB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#IO法トークナイザクラス"],"metadata":{"id":"QkkNDrvldH0B"}},{"cell_type":"markdown","source":["# 新しいセクション"],"metadata":{"id":"9Ql723EVeinh"}},{"cell_type":"code","source":["class NER_tokenizer(BertJapaneseTokenizer):\n","\n","    def encode_plus_tagged(self, text, entities, max_length):\n","        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく\n","        entities = sorted(entities, key=lambda x: x[\"span\"][0])\n","        splitted = []\n","        position = 0\n","        for entity in entities:\n","            start = entity[\"span\"][0]\n","            end = entity[\"span\"][1]\n","            label = entity[\"type_id\"]\n","\n","            # 固有表現でないものには0のラベルをつける\n","            splitted.append({\"text\":text[position:start], \"label\":0})\n","            # 固有表現には固有表現のタイプに対応するIDをラベルにつける\n","            splitted.append({\"text\":text[start:end], \"label\":label})\n","            position = end\n","        \n","        splitted.append({\"text\":text[position:], \"label\":0})\n","        # 長さが0の文字列は除く\n","        splitted = [s for s in splitted if s[\"text\"]]\n","\n","        # 分割されたそれぞれの文字列をトークン化し、ラベルをつける\n","        tokens = []\n","        labels = []\n","        for text_splitted in splitted:\n","            text = text_splitted[\"text\"]\n","            label = text_splitted[\"label\"]\n","            tokens_splitted = self.tokenize(text)\n","            labels_splitted = [label] * len(tokens_splitted)\n","            tokens.extend(tokens_splitted)\n","            labels.extend(labels_splitted)\n","\n","        # 符号化を行いBERTに入力できる形式にする\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        # input_idsをencodingに変換\n","        encoding = self.prepare_for_model(\n","            input_ids,\n","            max_length=max_length,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"tf\"\n","        )\n","        # 特殊トークン[CLS], [SEP]のラベルを0にする\n","        labels = [0] + labels[:max_length-2] + [0]\n","        #特殊トークン[PAD]のラベルを0にする\n","        labels = labels + [0] * (max_length - len(labels))\n","        encoding[\"labels\"] = labels\n","\n","        return encoding\n","\n","    def encode_plus_untagged(self, text, max_length=None):\n","        # 文章のトークン化を行い、それぞれのトークンと文章中の文字列を対応づける\n","        tokens = []\n","        tokens_original = []\n","        words = self.word_tokenizer.tokenize(text)\n","        for word in words:\n","            # 単語をサブワードに分割\n","            tokens_word = self.subword_tokenizer.tokenize(word)\n","            tokens.extend(tokens_word)\n","            if tokens_word[0] == \"[UNK]\":\n","                tokens_original.append(word)\n","            else:\n","                tokens_original.extend([\n","                    token.replace(\"##\", \"\") for token in tokens_word\n","                ])\n","\n","        # 各トークンの文書中での位置を調べる\n","        position = 0\n","        spans = []\n","        for token in tokens_original:\n","            len_token = len(token)\n","            while 1:\n","                if token != text[position:position+len_token]:\n","                    position += 1\n","                else:\n","                    spans.append([position, position+len_token])\n","                    position += len_token\n","                    break\n","\n","        # 符号化を行いBERTに入力できる形式にする\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        encoding = self.prepare_for_model(\n","            input_ids,\n","            max_length=max_length,\n","            padding=\"max_length\" if max_length else False,\n","            truncation=True if max_length else False,\n","            return_tensors=\"tf\"\n","        )\n","        sequence_length = len(encoding[\"input_ids\"])\n","        # 特殊トークン[CLS]に対するダミーのspanを追加\n","        spans = [[-1, -1]] + spans[:sequence_length-2]\n","        # 特殊トークン[SEP], [PAD]に対するダミーのspanを追加\n","        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n","\n","        return encoding, spans\n","\n","    def convert_bert_output_to_entities(self, text, labels, spans):\n","        # labels, spansから特殊トークンに来往する部分を取り除く\n","        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n","        spans = [span for span in spans if span[0] != -1]\n","\n","        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する\n","        entities = []\n","        for label, group in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n","            group = list(group)\n","            start = spans[group[0][0]][0]\n","            end = spans[group[-1][0]][1]\n","\n","            if label != 0:\n","                # ラベルが0以外ならば、新たな固有表現として追加\n","                entity = {\n","                    \"name\":text[start:end],\n","                    \"span\":[start, end],\n","                    \"type_id\":label\n","                }\n","                entities.append(entity)\n","\n","        return entities"],"metadata":{"id":"9wbEgToe5IcQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = NER_tokenizer.from_pretrained(MODEL_NAME)"],"metadata":{"id":"baDQlYgcKUSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"昨日のみらい事務所との打ち合わせは順調だった。\"\n","entities = [\n","            {\"name\":\"みらい事務所\", \"span\":[3, 9], \"type_id\":1}\n","]\n","encoding = tokenizer.encode_plus_tagged(text, entities, max_length=20)\n","print(encoding)"],"metadata":{"id":"idvUEx13KZsv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"騰訊の英語名はTencent Holdings Ltdである。\"\n","encoding, spans = tokenizer.encode_plus_untagged(text)\n","print(encoding)\n","print(spans)"],"metadata":{"id":"QwmFy7UtKv6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_predicted = [0, 1, 1, 0, 0, 0, 0, 1, 1, 1 ,1 ,1 ,1 ,1 ,1 ,1, 0, 0, 0, 0]\n","entities = tokenizer.convert_bert_output_to_entities(text, labels_predicted, spans)\n","print(entities)"],"metadata":{"id":"dHGvV06oTbbr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#トークン分類BERT"],"metadata":{"id":"0cNUcPKIdEyE"}},{"cell_type":"code","source":["bert = TFBertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=4)"],"metadata":{"id":"CvA3vde8XG0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"AさんはB大学に入学した。\"\n","encoding, spans = tokenizer.encode_plus_untagged(text)\n","print(encoding)"],"metadata":{"id":"JHjXhE_QdVbj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = bert(\n","    tf.reshape(encoding[\"input_ids\"], (1, 12)),\n","    tf.reshape(encoding[\"attention_mask\"], (1, 12)),\n","    tf.reshape(encoding[\"token_type_ids\"], (1, 12))\n",")\n","print(output)\n","scores = output.logits\n","labels_predicted = tf.argmax(scores[0], -1).numpy().tolist()"],"metadata":{"id":"v8YyKRpCdkEl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entities = tokenizer.convert_bert_output_to_entities(\n","    text, labels_predicted, spans\n",")\n","print(entities)"],"metadata":{"id":"xSfj0Idcd-Nq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#データセット"],"metadata":{"id":"7thGixg5kkFG"}},{"cell_type":"code","source":["!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset.git"],"metadata":{"id":"-4JbYdeYjqVU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = json.load(open(\"ner-wikipedia-dataset/ner.json\", \"r\"))"],"metadata":{"id":"AIZrUyYXlcUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset[10])"],"metadata":{"id":"HwFBcF6WUhEH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type_to_id = {entity[\"type\"] for data in dataset for entity in data[\"entities\"]}\n","type_to_id = {label:num+1 for num, label in enumerate(type_to_id)}\n","print(type_to_id)"],"metadata":{"id":"wSJkkBEBl9Rj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for data in dataset:\n","    data[\"text\"] = unicodedata.normalize(\"NFKC\", data[\"text\"])\n","    for entity in data[\"entities\"]:\n","        entity[\"type_id\"] = type_to_id[entity[\"type\"]]\n","        del entity[\"type\"]"],"metadata":{"id":"mNQ3HG-ul_bU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(dataset[10])"],"metadata":{"id":"XalRAUmNoG3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode_plus_tagged(dataset[10][\"text\"], dataset[10][\"entities\"], MAX_LENGTH))"],"metadata":{"id":"qFaP9VJemztD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random.shuffle(dataset)\n","num_dataset = len(dataset)\n","num_train = int(num_dataset * 0.6)\n","num_val = int(num_dataset * 0.2)\n","\n","dataset_train = dataset[:num_train]\n","dataset_val = dataset[num_train:num_train+num_val]\n","dataset_test = dataset[num_train+num_val:]"],"metadata":{"id":"02cekoCjmqiK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def to_train_feature(ds, tokenizer, max_length, num_labels):\n","    input_shape = (len(ds), max_length)\n","    output_shape = (len(ds), max_length)\n","\n","    input_ids = np.zeros(input_shape, np.int32)\n","    attention_mask = np.zeros(input_shape, np.int32)\n","    token_type_ids = np.zeros(input_shape, np.int32)\n","    labels = np.zeros(output_shape, np.int32)\n","\n","    for i, data in enumerate(ds):\n","\n","        encoding = tokenizer.encode_plus_tagged(data[\"text\"], data[\"entities\"], max_length)\n","        input_ids[i] = encoding[\"input_ids\"]\n","        attention_mask[i] = encoding[\"attention_mask\"]\n","        token_type_ids[i] = encoding[\"token_type_ids\"]\n","        labels[i] = encoding[\"labels\"]\n","        \n","\n","    return [input_ids, attention_mask, token_type_ids], labels"],"metadata":{"id":"XDrP6_VJ597b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, y_train = to_train_feature(dataset_train, tokenizer, MAX_LENGTH, len(type_to_id)+1)\n","X_val, y_val = to_train_feature(dataset_val, tokenizer, MAX_LENGTH, len(type_to_id)+1)\n","X_test, y_test = to_train_feature(dataset_test, tokenizer, MAX_LENGTH, len(type_to_id)+1)"],"metadata":{"id":"JOD11X9y2Hz9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#訓練開始"],"metadata":{"id":"Hg3p_7T6ygWk"}},{"cell_type":"code","source":["!rm -rf logs"],"metadata":{"id":"u2sfcthbaWhF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","log_dir = os.path.join('logs/', current_time)\n","ckpt_dir = os.path.join('ckpt/', current_time)"],"metadata":{"id":"ZebA-33wOItF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_tc = TFBertForTokenClassification.from_pretrained(\n","    MODEL_NAME, num_labels=len(type_to_id)+1\n",")"],"metadata":{"id":"gL1zNQl0w0oT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_tc.compile(\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n","    metrics=[\"accuracy\"]\n",")"],"metadata":{"id":"YjVfQE2KJIy1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 20\n","\n","callbacks = [\n","             tf.keras.callbacks.EarlyStopping(\n","                 monitor=\"val_loss\", mode=\"min\",\n","                 patience=5\n","             ),\n","             tf.keras.callbacks.TensorBoard(\n","                 log_dir=log_dir,\n","                 histogram_freq=1\n","             ),\n","             tf.keras.callbacks.ModelCheckpoint(\n","                 ckpt_dir,\n","                 save_best_only=True, save_weights_only=True\n","             )\n","]\n","\n","\n","history = bert_tc.fit(\n","    X_train, y_train, epochs=EPOCHS,\n","    batch_size=32,\n","    callbacks=callbacks,\n","    validation_data=(X_val, y_val), \n","    validation_batch_size=32\n",")"],"metadata":{"id":"_-L2xr688Fc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#性能評価"],"metadata":{"id":"YZanKdpbZf_3"}},{"cell_type":"code","source":["test_eval = bert_tc.evaluate(X_test, y_test)"],"metadata":{"id":"9r34x_yoZli6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(test_eval)"],"metadata":{"id":"-iSv2Xcwdvc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in [random.randint(0, len(X_test[0])) for _ in range(5)]:\n","    print(dataset_test[i][\"text\"])\n","    print(dataset_test[i][\"entities\"])\n","    output = bert_tc(\n","        [X_test[0][i].reshape((1,128)),\n","         X_test[1][i].reshape((1,128)),\n","         X_test[2][i].reshape((1,128))]\n","    )\n","    labels_predicted = tf.argmax(output.logits[0], axis=1)\n","    _, spans = tokenizer.encode_plus_untagged(dataset_test[i][\"text\"], MAX_LENGTH)\n","\n","    print(tokenizer.convert_bert_output_to_entities(\n","        dataset_test[i][\"text\"], labels_predicted.numpy(), spans\n","    ))"],"metadata":{"id":"r9BlfZWvXsHe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"MH2I8z0LDDfx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir logs/"],"metadata":{"id":"5KlO_h8HXmxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lFP3wHFAqwXy"},"execution_count":null,"outputs":[]}]}