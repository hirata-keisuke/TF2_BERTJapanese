{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pretrain.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN9txx6FRPQcvlOtMoP0sY8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#設定"],"metadata":{"id":"f7a5B_RPpGe1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zA73oXD1rP7A"},"outputs":[],"source":["!pip install -q transformers ipadic fugashi"]},{"cell_type":"code","source":["import tensorflow as tf\n","import json\n","import random\n","import matplotlib.pyplot as plt\n","import datetime\n","import numpy as np\n","\n","from tqdm import tqdm\n","from transformers import TFBertModel, BertJapaneseTokenizer"],"metadata":{"id":"JtPr_APA12Jk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BERT_MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n","MAX_LENGTH = 128"],"metadata":{"id":"eI06ja7N4WYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertJapaneseTokenizer.from_pretrained(BERT_MODEL_NAME)"],"metadata":{"id":"VuBKEPv56Ssy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#データのダウンロード"],"metadata":{"id":"WUua_wwXpUBN"}},{"cell_type":"code","source":["!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/chABSA-dataset.zip ."],"metadata":{"id":"E0w2GhH1LbXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip -q chABSA-dataset.zip"],"metadata":{"id":"IQab4uaXqCK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["polarity_to_label = {\"negative\":0, \"neutral\":1, \"positive\":2}\n","dataset = list()\n","filepaths = tf.io.gfile.glob(\"chABSA-dataset/*.json\")\n","random.shuffle(filepaths)\n","\n","for filepath in filepaths:\n","    data = json.load(open(filepath, \"r\"))\n","    # 各文の極性をリストにまとめる。\n","    for sentence in data[\"sentences\"]:\n","        text = sentence[\"sentence\"]\n","        labels = [0, 0, 0]\n","        # 極性を持つ語を含めば、その文章はその極性を持つこととする。\n","        for opinion in sentence[\"opinions\"]:\n","            labels[polarity_to_label[opinion[\"polarity\"]]] = 1\n","        example = {\"text\":text, \"labels\":labels}\n","        dataset.append(example)"],"metadata":{"id":"pdqE1QZXrefl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_examples = len(dataset)\n","encoded_data = {\"input_ids\":list(), \"attention_mask\":list(), \"token_type_ids\":list(), \"labels\":list()}\n","for example in dataset:\n","    encoded = tokenizer(\n","        example[\"text\"], max_length=MAX_LENGTH,\n","        padding=\"max_length\", truncation=True,\n","        return_tensors=\"tf\"\n","    )\n","    for k, v in encoded.items():\n","        encoded_data[k].append(v[0])\n","    encoded_data[\"labels\"].append(example[\"labels\"])"],"metadata":{"id":"vIxeibdmz3ql"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoded_dataset = tf.data.Dataset.from_tensor_slices(encoded_data)"],"metadata":{"id":"vz8iUcOx1v5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = encoded_dataset.take(int(num_examples*0.6))\n","test_dataset = encoded_dataset.skip(int(num_examples*0.6)).take(int(num_examples*0.2))\n","valid_dataset = encoded_dataset.skip(int(num_examples*0.8)).take(-1)"],"metadata":{"id":"MythSYS62ACG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#マルチラベリングモデルの定義"],"metadata":{"id":"EY2c_ERapN-g"}},{"cell_type":"code","source":["def create_classifier(\n","    num_labels, \n","    sequence_length, \n","    bert_model_name=BERT_MODEL_NAME, \n","    bert_trainable=False, \n","    activation=None):\n","\n","    input_ids = tf.keras.layers.Input(shape=(sequence_length,), dtype=tf.int32, name=\"input_ids\")\n","    attention_mask = tf.keras.layers.Input(shape=(sequence_length,), dtype=tf.int32, name=\"attention_mask\")\n","    token_type_ids = tf.keras.layers.Input(shape=(sequence_length,), dtype=tf.int32, name=\"token_type_ids\")\n","\n","    bert = TFBertModel.from_pretrained(bert_model_name, name=\"bert\")\n","    bert.layers[0].trainable = bert_trainable\n","    multiply = tf.keras.layers.Multiply(name=\"multiply\")\n","    dense = tf.keras.layers.Dense(num_labels, name=\"dense\", activation=activation)\n","\n","    x = bert.bert([input_ids, attention_mask, token_type_ids])\n","\n","    # BERTの各トークンに対する出力を使う\n","    last_hidden_state = x.last_hidden_state\n","\n","    # attention_maskが1の、[SPAN]トークンでない出力を平均する\n","    mask = tf.tile(\n","        tf.expand_dims(attention_mask, 2),\n","        [1, 1, bert.config.to_dict()[\"hidden_size\"]]\n","    )\n","    mask = tf.cast(mask, tf.float32)\n","    x = multiply([last_hidden_state, mask])\n","    x = tf.math.reduce_sum(x, axis=1) / tf.math.reduce_sum(mask, axis=1)\n","\n","    x = dense(x)\n","\n","    return tf.keras.Model(inputs=[input_ids, attention_mask, token_type_ids], outputs=x)"],"metadata":{"id":"sUuk9yaKyfxr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#モデルの学習"],"metadata":{"id":"8UFZhF8U4SKa"}},{"cell_type":"code","source":["EPOCHS = 3\n","BATCHSIZE = 32\n","STEPS_PER_EPOCH = len(train_dataset)//BATCHSIZE + 1"],"metadata":{"id":"J6fwcvvy9utV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset_batched = train_dataset.shuffle(100).repeat().batch(BATCHSIZE)"],"metadata":{"id":"c9nC7RNPD5rt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##BERTの中身を学習させる"],"metadata":{"id":"tC-iBGNFN7Kw"}},{"cell_type":"code","source":["!rm -rf logs"],"metadata":{"id":"J9yFiR43r28O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multi_labeling = create_classifier(3, MAX_LENGTH, BERT_MODEL_NAME, True, \"sigmoid\")"],"metadata":{"id":"qBYJgUFB3vhF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["d = next(iter(train_dataset.batch(10)))\n","multi_labeling([d[\"input_ids\"], d[\"attention_mask\"], d[\"token_type_ids\"]])"],"metadata":{"id":"Pc6DJ0OMCjEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n","acc_metrics_obj = tf.keras.metrics.BinaryAccuracy()\n","loss_metrics_obj = tf.keras.metrics.BinaryCrossentropy()"],"metadata":{"id":"iBqxrlQM5New"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n","valid_log_dir = 'logs/gradient_tape/' + current_time + '/valid'\n","train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n","valid_summary_writer = tf.summary.create_file_writer(valid_log_dir)"],"metadata":{"id":"iqgUEYaCyI-W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 訓練ステップ関数\n","@tf.function\n","def train_step(inputs, loss_fn, optimizer, acc_metrics_obj, loss_metrics_obj):\n","    with tf.GradientTape() as tape:\n","        scores = multi_labeling(\n","            [inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]]\n","        )\n","        loss = loss_fn(inputs[\"labels\"], scores)\n","\n","    grads = tape.gradient(loss, multi_labeling.trainable_weights)\n","    optimizer.apply_gradients(list(zip(\n","        grads, multi_labeling.trainable_weights)\n","    ))\n","\n","    acc_metrics_obj.update_state(inputs[\"labels\"], scores)\n","    loss_metrics_obj.update_state(inputs[\"labels\"], scores)\n","\n","# 検証ステップ関数\n","@tf.function\n","def eval_step(inputs, acc_metrics_obj, loss_metrics_obj):\n","    scores = multi_labeling(\n","            [inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]]\n","        )\n","\n","    acc_metrics_obj.update_state(inputs[\"labels\"], scores)\n","    loss_metrics_obj.update_state(inputs[\"labels\"], scores)"],"metadata":{"id":"3PKnxjPL4fAb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# メトリクス保存用\n","metrics = dict(\n","    acc=list(), loss=list(),\n","    val_acc=list(), val_loss=list()\n",")\n","\n","train_iter = iter(train_dataset_batched)\n","for epoch in range(EPOCHS):    \n","\n","    print(f\"{epoch+1} EPOCH START\")\n","    for step in tqdm(range(STEPS_PER_EPOCH)):\n","        inputs = next(train_iter)\n","        # 訓練の1ステップ\n","        train_step(inputs, loss_fn, optimizer, acc_metrics_obj, loss_metrics_obj)\n","        \n","    # ステップトータルの損失として保存する\n","    metrics[\"acc\"].append(acc_metrics_obj.result().numpy())\n","    metrics[\"loss\"].append(loss_metrics_obj.result().numpy())\n","\n","    # 訓練結果をTensorboard用に保存する\n","    with train_summary_writer.as_default():\n","        tf.summary.scalar('accuracy', acc_metrics_obj.result(), step=epoch)\n","        tf.summary.scalar('loss', loss_metrics_obj.result(), step=epoch)\n","\n","        for layer in multi_labeling.layers:\n","            for trainable_variable in layer.trainable_variables:\n","                tf.summary.histogram(\n","                    trainable_variable.name,\n","                    trainable_variable,\n","                    step=epoch)\n","\n","    # 保存したメトリクスを消去する\n","    acc_metrics_obj.reset_state()\n","    loss_metrics_obj.reset_state()\n","\n","    # 検証ループ\n","    for inputs in valid_dataset.batch(BATCHSIZE):\n","        eval_step(inputs, acc_metrics_obj, loss_metrics_obj)\n","    \n","    metrics[\"val_acc\"].append(acc_metrics_obj.result().numpy())\n","    metrics[\"val_loss\"].append(loss_metrics_obj.result().numpy())\n","\n","    with valid_summary_writer.as_default():\n","        tf.summary.scalar('accuracy', acc_metrics_obj.result(), step=epoch)\n","        tf.summary.scalar('loss', loss_metrics_obj.result(), step=epoch)\n","\n","    acc_metrics_obj.reset_state()\n","    loss_metrics_obj.reset_state()\n","\n","    print(\n","        f\"acc:{metrics['acc'][-1]:.4f}, loss:{metrics['loss'][-1]:.4f}, val_acc:{metrics['val_acc'][-1]:.4f}, val_loss:{metrics['val_loss'][-1]:.4f}\\t\"\n","    )"],"metadata":{"id":"YylhU-0R8XBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n","ax.plot(range(EPOCHS), metrics[\"acc\"], label=\"acc\")\n","ax.plot(range(EPOCHS), metrics[\"val_acc\"], label=\"val_acc\")\n","\n","ay = ax.twinx()\n","ay.plot(range(EPOCHS), metrics[\"loss\"], label=\"loss\", linestyle=\"--\")\n","ay.plot(range(EPOCHS), metrics[\"val_loss\"], label=\"val_loss\", linestyle=\"--\")\n","\n","ax.legend(loc=\"center right\")\n","ay.legend(loc=\"lower right\")\n","plt.show()"],"metadata":{"id":"8GjqZyyGBNK2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for inputs in test_dataset.batch(BATCHSIZE):\n","    pred = multi_labeling(\n","        [inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]]\n","    )\n","\n","    acc_metrics_obj.update_state(inputs[\"labels\"], pred)\n","    loss_metrics_obj.update_state(inputs[\"labels\"], pred)\n","\n","print(f\"accuracy for test dataset : {acc_metrics_obj.result().numpy():.4f}\\n\")\n","print(f\"loss for test dataset : {loss_metrics_obj.result().numpy():.4f}\\n\")\n","\n","acc_metrics_obj.reset_state()\n","loss_metrics_obj.reset_state()"],"metadata":{"id":"gbs4I87PMTdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["binary = tf.keras.losses.BinaryCrossentropy()\n","categ = tf.keras.losses.CategoricalCrossentropy()\n","acc = tf.keras.metrics.BinaryAccuracy()\n","for inputs in test_dataset.shuffle(100).batch(5).take(1):\n","    for a,b,c,d in zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"],inputs[\"labels\"]):\n","        print(tokenizer.decode(a, skip_special_tokens=True).replace(\" \", \"\"))\n","        score = multi_labeling(\n","            [np.array(a).reshape((1,128)), np.array(b).reshape((1,128)), np.array(c).reshape((1,128))]\n","        )\n","        print(\"# predict\")\n","        print(score.numpy()[0])\n","        print(\"# label\")\n","        print(d.numpy())\n","        print()"],"metadata":{"id":"kYncKg-1abhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multi_labeling.save_weights(\"ckpt/bert_not_trainable\")"],"metadata":{"id":"IT0n7GgCNCda"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_model = create_classifier(3, MAX_LENGTH)\n","loaded_model.load_weights(\"ckpt/bert_not_trainable\")\n","\n","for inputs in test_dataset.batch(BATCHSIZE):\n","    pred = loaded_model(\n","        [inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"]]\n","    )\n","\n","    acc_metrics_obj.update_state(inputs[\"labels\"], pred)\n","    loss_metrics_obj.update_state(inputs[\"labels\"], pred)\n","\n","print(f\"accuracy for test dataset : {acc_metrics_obj.result().numpy():.4f}\\n\")\n","print(f\"loss for test dataset : {loss_metrics_obj.result().numpy():.4f}\\n\")\n","\n","acc_metrics_obj.reset_state()\n","loss_metrics_obj.reset_state()"],"metadata":{"id":"rbIHLzA9NZ1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#TensorBoardのためのログを出力させる"],"metadata":{"id":"HiFb6eG5e8na"}},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"dxuifRBEjZb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir logs"],"metadata":{"id":"bnM3-TIkjekU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"5CmU6Ys0HgAw"},"execution_count":null,"outputs":[]}]}