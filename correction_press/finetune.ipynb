{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"finetune.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNf9Ee4aed6yzKnuQ743e7d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#設定"],"metadata":{"id":"tVVowErIW84a"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5gzXdoGzVJdF"},"outputs":[],"source":["%%capture\n","!pip install transformers fugashi ipadic"]},{"cell_type":"code","source":["import tensorflow as tf\n","import os\n","import datetime\n","import random\n","import unicodedata\n","import pandas as pd\n","import numpy as np\n","\n","from transformers import BertJapaneseTokenizer, TFBertForMaskedLM"],"metadata":{"id":"UH1kAZMVVTqr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n","MAX_LENGTH = 128"],"metadata":{"id":"fMCbL1aEVm_Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#トークナイザ"],"metadata":{"id":"u-w3_mCfW-Xz"}},{"cell_type":"code","source":["class SC_tokenizer(BertJapaneseTokenizer):\n","\n","    def encode_plus_tagged(self, wrong_text, correct_text, max_length=128):\n","        encoding = self(\n","            wrong_text, max_length=max_length, \n","            padding=\"max_length\", truncation=True, return_tensors=\"tf\"\n","        )\n","        encoding_correct = self(\n","            correct_text, max_length=max_length, \n","            padding=\"max_length\", truncation=True, return_tensors=\"tf\"\n","        )\n","        encoding[\"label\"] = encoding_correct[\"input_ids\"]\n","        return encoding\n","\n","    def encode_plus_untagged(self, text, max_length=None):\n","        tokens = []\n","        tokens_original = []\n","        words = self.word_tokenizer.tokenize(text)\n","        for word in words:\n","            tokens_word = self.subword_tokenizer.tokenize(word)\n","            tokens.extend(tokens_word)\n","            if tokens_word[0] == \"[UNK]\":\n","                tokens_original.append(word)\n","            else:\n","                tokens_original.extend([\n","                                        token.replace(\"##\", \"\") for token in tokens_word\n","                ])\n","\n","        position = 0\n","        spans = []\n","        for token in tokens_original:\n","            length = len(token)\n","            while 1:\n","                if token != text[position:position+length]:\n","                    position += 1\n","                else:\n","                    spans.append([position, position+length])\n","                    position += length\n","                    break\n","\n","        input_ids = self.convert_tokens_to_ids(tokens)\n","        encoding = self.prepare_for_model(\n","            input_ids, max_length=max_length,\n","            padding=\"max_length\" if max_length else False,\n","            trunncation=True if max_length else False,\n","            return_tensors=\"tf\"\n","        )\n","        sequence_length = len(encoding[\"input_ids\"])\n","        spans = [[-1, -1]] + spans[:sequence_length-2]\n","        spans = spans + [[-1, -1]] * (sequence_length - len(spans))\n","\n","        return encoding, spans\n","    \n","    def convert_bert_output_to_text(self, text, labels, spans):\n","        assert len(spans) == len(labels)\n","\n","        labels = [label for label, span in zip(labels, spans) if span[0] != -1]\n","        spans = [span for span in spans if span[0] != -1]\n","\n","        predicted_text = \"\"\n","        position = 0\n","        for label, span in zip(labels, spans):\n","            start, end = span\n","            if position != start:\n","                predicted_text += text[position:start]\n","            predicted_token = self.convert_ids_to_tokens(label)\n","            predicted_token = predicted_token.replace(\"##\", \"\")\n","            predicted_token = unicodedata.normalize(\"NFKC\", predicted_token)\n","            predicted_text += predicted_token\n","            position = end\n","        \n","        return predicted_text"],"metadata":{"id":"quqt2-YvV2s7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = SC_tokenizer.from_pretrained(MODEL_NAME)"],"metadata":{"id":"IZA_3gT_aaig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wrong_text = \"優勝トロフィーを変換した\"\n","correct_text = \"優勝トロフィーを返還した\"\n","encoding = tokenizer.encode_plus_tagged(\n","    wrong_text, correct_text, max_length=12\n",")\n","print(encoding)"],"metadata":{"id":"VsLlxiQWagfd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoding, spans = tokenizer.encode_plus_untagged(wrong_text)\n","print(\"# encoding\")\n","print(encoding)\n","print(\"# spans\")\n","print(spans)"],"metadata":{"id":"leRtUf3raxDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_labels = [2, 759, 18204, 11, 8274, 15, 10, 3]\n","predicted_text = tokenizer.convert_bert_output_to_text(\n","    wrong_text, predicted_labels, spans\n",")\n","print(predicted_text)"],"metadata":{"id":"AQlDBXQMbMaa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#BertForMaskedMLM"],"metadata":{"id":"Plm3xWH6dyFA"}},{"cell_type":"code","source":["bert_mlm = TFBertForMaskedLM.from_pretrained(MODEL_NAME)"],"metadata":{"id":"jTu72awIbjER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"優勝トロフィーを変換した。\"\n","\n","encoding, spans = tokenizer.encode_plus_untagged(text)\n","\n","output = bert_mlm(\n","    tf.reshape(encoding[\"input_ids\"], (1, 9)),\n","    tf.reshape(encoding[\"attention_mask\"], (1, 9)),\n","    tf.reshape(encoding[\"token_type_ids\"], (1, 9))\n",")\n","scores = output.logits\n","labels_predicted = tf.argmax(scores[0], axis=-1).numpy().tolist()\n","\n","predicted_text = tokenizer.convert_bert_output_to_text(\n","    text, labels_predicted, spans\n",")\n","print(predicted_text)"],"metadata":{"id":"Wo85VV5vd9Dr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wrong_texts = [\"優勝トロフィーを変換した。\", \"人と森は強制している。\"]\n","correct_texts = [\"優勝トロフィーを返還した。\" , \"人と森は共生している。\"]\n","\n","max_length = 32\n","input_shape = (len(wrong_texts), max_length)\n","dataset = {\n","    \"input_ids\" : np.zeros(input_shape, dtype=np.int32), \n","    \"attention_mask\" : np.zeros(input_shape, dtype=np.int32),\n","    \"token_type_ids\" : np.zeros(input_shape, dtype=np.int32)\n","}\n","\n","for i in range(len(wrong_texts)):\n","    encoding = tokenizer.encode_plus_tagged(wrong_texts[i], correct_texts[i], max_length)\n","    for k in dataset.keys():\n","        dataset[k][i] = encoding[k].numpy()[0]\n","\n","output = bert_mlm(**dataset)\n","loss = output.loss"],"metadata":{"id":"Dvub3Ix4eK5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#データセット\n","\n","漢字の誤変換だけを対象にする"],"metadata":{"id":"vUC6o3TUjx0E"}},{"cell_type":"code","source":["!curl -L -o JWTD.tar.gz https://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=https://nlp.ist.i.kyoto-u.ac.jp/nl-resource/JWTD/jwtd_v2.0.tar.gz&name=JWTDv2.0.tar.gz\n","!tar zxvf JWTD.tar.gz"],"metadata":{"id":"7W3pepoWi9RE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df = pd.read_json(\"jwtd_v2.0/train.jsonl\",  orient='records', lines=True)\n","train_df = train_df.sample(frac=1, ignore_index=True)"],"metadata":{"id":"iLelDgWxklZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df = pd.read_json(\"jwtd_v2.0/test.jsonl\",  orient='records', lines=True)"],"metadata":{"id":"OPa9i-E8ay6o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df.head()"],"metadata":{"id":"LIZvOdUUlpqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.head(1)"],"metadata":{"id":"xsIeyqZ7a_1_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["category_types = set()\n","for i in range(len(train_df)):\n","    if isinstance(train_df.loc[i, \"diffs\"][0][\"category\"], list):\n","        for category in train_df.loc[i, \"diffs\"][0][\"category\"]:\n","            if isinstance(category, str):\n","                category_types.add(category)\n","    else:\n","        category_types.add(train_df.loc[i, \"diffs\"][0][\"category\"])"],"metadata":{"id":"KMzu7oc0ojCD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(category_types)"],"metadata":{"id":"MejeAgGkMA3z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pickup_kanji_conversion(dataset, tokenizer):\n","    correct_texts = []\n","    wrong_texts = []\n","\n","    for i in range(len(dataset)):\n","        if \"kanji-conversion_a\" in dataset.loc[i, \"diffs\"][0][\"category\"] or\\\n","         \"kanji-conversion_b\" in dataset.loc[i, \"diffs\"][0][\"category\"]:\n","            correct_tokens = tokenizer.tokenize(dataset.loc[i, \"post_text\"])\n","            wrong_tokens = tokenizer.tokenize(dataset.loc[i, \"pre_text\"])\n","\n","            if len(correct_tokens) == len(wrong_tokens):\n","                correct_texts.append(\n","                    unicodedata.normalize(\"NFKC\", dataset.loc[i, \"post_text\"])\n","                )\n","                wrong_texts.append(\n","                    unicodedata.normalize(\"NFKC\", dataset.loc[i, \"pre_text\"])\n","                )\n","\n","    return wrong_texts, correct_texts"],"metadata":{"id":"mfHWL9TuMs-w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wrong_texts, correct_texts = pickup_kanji_conversion(train_df, tokenizer)\n","wrong_texts_test, correct_texts_test = pickup_kanji_conversion(test_df, tokenizer)"],"metadata":{"id":"RvlyYMz3ZG3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_train = int(len(wrong_texts) * 0.8)"],"metadata":{"id":"LizHR7Koc9Mo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wrong_texts_train = wrong_texts[:num_train]\n","wrong_texts_valid = wrong_texts[num_train:]\n","correct_texts_train = correct_texts[:num_train]\n","correct_texts_valid = correct_texts[num_train:]"],"metadata":{"id":"7aTnNEjGh3lj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def to_input(texts, tokenizer, max_length):\n","    \n","    encodings = tokenizer(\n","        texts, max_length=max_length,\n","        padding=\"max_length\", truncation=True,\n","        return_tensors=\"tf\"\n","    )\n","\n","    return [encodings[\"input_ids\"], encodings[\"attention_mask\"], encodings[\"token_type_ids\"]]"],"metadata":{"id":"bId3bCh8nCLI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def to_output(texts, tokenizer, max_length):\n","    input_shape = (len(texts), max_length)\n","    encodings = np.zeros(input_shape, dtype=np.int32)\n","\n","    for i in range(len(texts)):\n","        encodings[i] = tokenizer.encode(\n","            texts[i],\n","            max_length=max_length, padding=\"max_length\",\n","            truncation=True, return_tensors=\"tf\"\n","        ).numpy()\n","\n","    return encodings"],"metadata":{"id":"2DxbZ0hjogCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = to_input(wrong_texts_train, tokenizer, MAX_LENGTH)\n","y_train = to_output(correct_texts_train, tokenizer, MAX_LENGTH)\n","X_valid = to_input(wrong_texts_valid, tokenizer, MAX_LENGTH)\n","y_valid = to_output(correct_texts_valid, tokenizer, MAX_LENGTH)\n","X_test = to_input(wrong_texts_test, tokenizer, MAX_LENGTH)\n","y_test = to_output(correct_texts_test, tokenizer, MAX_LENGTH)"],"metadata":{"id":"z92gOGSikLFP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_valid[0].shape"],"metadata":{"id":"-UtaIcCAt0d9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_valid.shape"],"metadata":{"id":"FdcxQn28vBo1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ファインチューニング"],"metadata":{"id":"dPuNGBe_jI88"}},{"cell_type":"code","source":["!rm -rf logs"],"metadata":{"id":"_6eg9MK0jOaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","log_dir = os.path.join('logs/', current_time)\n","ckpt_dir = os.path.join('ckpt/', current_time)"],"metadata":{"id":"q_v9-4s2jcJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_mlm = TFBertForMaskedLM.from_pretrained(MODEL_NAME)"],"metadata":{"id":"3wA8XF0SiPyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_mlm.compile(\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n","    metrics=[\"accuracy\"]\n",")"],"metadata":{"id":"q-alD0U6jm37"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 1\n","\n","callbacks = [\n","             tf.keras.callbacks.EarlyStopping(\n","                 monitor=\"val_loss\", mode=\"min\",\n","                 patience=5\n","             ),\n","             tf.keras.callbacks.TensorBoard(\n","                 log_dir=log_dir,\n","                 histogram_freq=1\n","             ),\n","             tf.keras.callbacks.ModelCheckpoint(\n","                 ckpt_dir,\n","                 save_best_only=True, save_weights_only=True\n","             )\n","]\n","\n","\n","history = bert_mlm.fit(\n","    X_train, y_train, \n","    epochs=EPOCHS, batch_size=64,\n","    callbacks=callbacks,\n","    validation_data=(X_valid, y_valid), \n","    validation_batch_size=32\n",")"],"metadata":{"id":"pz1NV_H4j5zz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%load_ext tensorboard"],"metadata":{"id":"OsFXldB13vlq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir {log_dir}"],"metadata":{"id":"PY_g6c8L3uxD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#性能評価"],"metadata":{"id":"sZjps4gk03MT"}},{"cell_type":"code","source":["_, spans = tokenizer.encode_plus_untagged(\n","    wrong_texts_test[0], max_length=MAX_LENGTH\n",")"],"metadata":{"id":"IKFD4LLJ2eYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = bert_mlm([\n","                   tf.reshape(X_test[0][0], (1,128)), \n","                   tf.reshape(X_test[1][0], (1,128)), \n","                   tf.reshape(X_test[2][0], (1,128))\n","]).logits\n","predicted_labels = tf.argmax(output, axis=2)[0].numpy().tolist()"],"metadata":{"id":"hrb9FioOmO21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"入力：{wrong_texts_test[0]}\")\n","print(f\"予想：{tokenizer.convert_bert_output_to_text(wrong_texts_test[0], predicted_labels, spans)}\")\n","print(f\"正解：{correct_texts_test[0]}\")"],"metadata":{"id":"ErU5NG9229d4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"fSG-M3kRDsIU"},"execution_count":null,"outputs":[]}]}