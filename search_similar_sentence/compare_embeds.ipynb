{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"compare_embeds.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNST34P1y7aVv7f0y3LLbNQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#設定"],"metadata":{"id":"xonXInhseaNk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Trl4JZZddRym"},"outputs":[],"source":["%%capture\n","!pip install transformers ipadic fugashi"]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","\n","from transformers import BertJapaneseTokenizer, TFBertModel"],"metadata":{"id":"H_qODJGpdviB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n","MAX_LENGTH = 256"],"metadata":{"id":"shPPH84xd8bj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#データセット"],"metadata":{"id":"-ZPVDbUFeU6m"}},{"cell_type":"code","source":["!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n","!tar -zxf ldcc-20140209.tar.gz"],"metadata":{"id":"VKuSq0S0eCDM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#ベクトル化"],"metadata":{"id":"1sAoxkIGetpe"}},{"cell_type":"code","source":["filepaths = tf.io.gfile.glob(\"./text/*/*-*.txt\")"],"metadata":{"id":"xM649-T-envd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["category = list({filepath.split(\"/\")[2] for filepath in filepaths})"],"metadata":{"id":"PfR8vY8he8Fm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","model = TFBertModel.from_pretrained(MODEL_NAME)"],"metadata":{"id":"H7vLcBCGfCTY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[CLS]トークン、もしくは各トークンの出力を平均を文章ベクトルにする。"],"metadata":{"id":"HuHxHdXJgBgs"}},{"cell_type":"code","source":["labels = list()\n","sentence_vectors_cls = np.zeros((len(filepaths), model.config.to_dict()[\"hidden_size\"]))\n","sentence_vectors_avg = np.zeros((len(filepaths), model.config.to_dict()[\"hidden_size\"]))\n","\n","for i, filepath in enumerate(filepaths):\n","    labels.append(filepath.split(\"/\")[2])\n","\n","texts = list()\n","for i in range(len(filepaths)//500):\n","    start = i*500\n","    end = (i+1)*500\n","    print(f\"{start}-{end}\")\n","    \n","    for j in range(start, end):\n","        with open(filepaths[j], \"r\") as f:\n","            text = \"\".join(f.readlines()[3:])\n","        texts.append(text)\n","\n","    encode = tokenizer(\n","        texts, max_length=MAX_LENGTH,\n","        padding=\"max_length\", truncation=True,\n","        return_tensors=\"tf\"\n","    )\n","\n","    output = model(**encode)\n","    \n","    mask = tf.tile(\n","        tf.expand_dims(encode[\"attention_mask\"], 2), \n","        [1, 1, model.config.to_dict()[\"hidden_size\"]]\n","    )\n","    mask = tf.cast(mask, tf.float32)\n","\n","    avg = tf.math.reduce_sum(output.last_hidden_state*mask, 1) / tf.math.reduce_sum(mask, 1)\n","\n","    sentence_vectors_cls[start:end] = output.pooler_output.numpy()\n","    sentence_vectors_avg[start:end] = avg.numpy()\n","    texts = list()\n","\n","for j in range(end, len(filepaths)):\n","    with open(filepaths[j], \"r\") as f:\n","        text = \"\".join(f.readlines()[3:])\n","    texts.append(text)\n","print(f\"{end}-{len(filepaths)}\")\n","\n","encode = tokenizer(\n","    texts, max_length=MAX_LENGTH,\n","    padding=\"max_length\", truncation=True,\n","    return_tensors=\"tf\"\n",")\n","output = model(**encode)\n","\n","mask = tf.tile(\n","    tf.expand_dims(encode[\"attention_mask\"], 2), \n","    [1, 1, model.config.to_dict()[\"hidden_size\"]]\n",")\n","mask = tf.cast(mask, tf.float32)\n","\n","avg = tf.math.reduce_sum(output.last_hidden_state*mask, 1) / tf.math.reduce_sum(mask, 1)\n","\n","sentence_vectors_cls[end:] = output.pooler_output.numpy()\n","sentence_vectors_avg[end:] = avg.numpy()"],"metadata":{"id":"M45OpIF1hA-p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_compressed_features(vectors, categories, paths):\n","    fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(10, 8))\n","    axes = axes.ravel()\n","\n","    for ax, ctg in zip(axes, categories):\n","        ax.scatter(vectors[:, 0], vectors[:, 1], c=\"gray\", alpha=0.6, s=5)\n","\n","        indices = list()\n","        for i in range(len(paths)):\n","            if ctg in paths[i]:\n","                indices.append(i)\n","\n","        ax.scatter(vectors[indices, 0], vectors[indices, 1], c=\"red\", s=5)\n","        ax.set_title(ctg)\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"_DzjG4VSEtLO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#PCA(主成分分析)"],"metadata":{"id":"ixCCKKdbmAgW"}},{"cell_type":"code","source":["cls_pca = PCA(n_components=2).fit_transform(sentence_vectors_cls)\n","avg_pca = PCA(n_components=2).fit_transform(sentence_vectors_avg)"],"metadata":{"id":"Q0YZPBiyi_6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_compressed_features(cls_pca, category, filepaths)"],"metadata":{"id":"bQ3afRnpFePt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_compressed_features(avg_pca, category, filepaths)"],"metadata":{"id":"IcO1vPLImuK_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#t-SNE"],"metadata":{"id":"IAtde1LSFp3d"}},{"cell_type":"code","source":["cls_tsne = TSNE(n_components=2).fit_transform(sentence_vectors_cls)\n","avg_tsne = TSNE(n_components=2).fit_transform(sentence_vectors_avg)"],"metadata":{"id":"aSM5IPrZ_Qxl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_compressed_features(cls_tsne, category, filepaths)"],"metadata":{"id":"tjAb57icFzuU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_compressed_features(avg_tsne, category, filepaths)"],"metadata":{"id":"e5KZxQKgGMW-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["t-SNEの特徴量抽出の方が、PCAよりもカテゴリごとに分けられているように見える。\n","\n","t-SNEでは、[CLS]トークンでもトークンの平均でも違いはなさそう。"],"metadata":{"id":"qqaIGyZ5Gz_8"}},{"cell_type":"code","source":[""],"metadata":{"id":"4LNpEUNuGsiZ"},"execution_count":null,"outputs":[]}]}